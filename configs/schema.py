# ==============================================================================
# FAST CLARA ABSA: BATCH STRUCTURE SPECIFICATION
# ==============================================================================
# B: Batch Size (например, 8, 16 или 32)
# L_enc: Max Encoder Length (конфиг: 128)
# L_dec: Max Decoder Length (конфиг: 128 или 64)
# ==============================================================================

BATCH_SCHEMA = {
    # --- ВХОД ДЛЯ ЭНКОДЕРА (СЖАТИЕ ТЕКСТА) ---
    "enc_input_ids":  torch.LongTensor,   # Размер: [B, L_enc]
    # Содержит: [Токены исходного текста] + [Токены памяти M0..Mk] + [Padding]

    "enc_mask":       torch.BoolTensor,   # Размер: [B, L_enc]
    # Содержит: 1 для реальных токенов, 0 для паддинга

    # --- ВХОД ДЛЯ ДЕКОДЕРА (РАССУЖДЕНИЕ/ГЕНЕРАЦИЯ) ---
    "dec_input_ids":  torch.LongTensor,   # Размер: [B, L_dec]
    # Содержит: [Токены памяти M0..Mk] + [Токены промпта задачи] + [Токены ответа] + [Padding]

    "dec_mask":       torch.BoolTensor,   # Размер: [B, L_dec]
    # Содержит: 1 для памяти, промпта и ответа. 0 для паддинга

    # --- ТАРГЕТЫ ДЛЯ ВЫЧИСЛЕНИЯ LOSS ---
    "labels":         torch.LongTensor,   # Размер: [B, L_dec]
    # Содержит: -100 на всех позициях, КРОМЕ токенов ответа (Sentiment/Text/JSON)
    # Позволяет вычислять CrossEntropy только по полезной генерации.

    # --- МЕТАДАННЫЕ ---
    "task":           list[str],          # Размер: [B]
    # Содержит: Строковые идентификаторы ("rec", "ext", "reason") для каждой строки батча
}