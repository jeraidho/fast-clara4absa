{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6049286-02cd-4f0b-91fc-4177c814cac1",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8acc91d-dfbf-49c4-8354-bbcdbfcad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07039259-9c3d-46e2-86e2-f62d4402c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70cfbc3-d10f-4932-8af5-cd592783c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0524e3-221f-40b7-b3c8-79aa3b84841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4743b385-88fa-4aa1-bb11-441cf47c1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    if 'clara_model' in globals():\n",
    "        del clara_model\n",
    "    if 'peft_model' in globals():\n",
    "        del peft_model\n",
    "    if 'base_model' in globals():\n",
    "        del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2d05d0-9316-4a61-895a-30e92b967ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n",
      "bitsandbytes library load error: Configured CUDA binary not found at /home/jovyan/.conda/envs/byakubson-nlp-absa/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.conda/envs/byakubson-nlp-absa/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 320, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/.conda/envs/byakubson-nlp-absa/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 288, in get_native_library\n",
      "    raise RuntimeError(f\"Configured {BNB_BACKEND} binary not found at {cuda_binary_path}\")\n",
      "RuntimeError: Configured CUDA binary not found at /home/jovyan/.conda/envs/byakubson-nlp-absa/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "Epoch 0: 100%|██████████| 4945/4945 [1:14:41<00:00,  1.10it/s, L=0.0834, A=0.84, B=0.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation for epoch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.2124, Val ABSA: 0.5186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/byakubson-nlp-absa/lib/python3.11/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: checkpoint is saved in checkpoints/clara_epoch_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4945/4945 [1:14:43<00:00,  1.10it/s, L=0.855, A=0.68, B=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation for epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8852, Val ABSA: 0.4671\n",
      "Epoch 1: checkpoint is saved in checkpoints/clara_epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4945/4945 [1:14:43<00:00,  1.10it/s, L=0.238, A=0.52, B=0.64] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation for epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8079, Val ABSA: 0.4671\n",
      "Epoch 2: checkpoint is saved in checkpoints/clara_epoch_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4945/4945 [1:14:39<00:00,  1.10it/s, L=0.443, A=0.36, B=0.82] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation for epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7310, Val ABSA: 0.4696\n",
      "Epoch 3: checkpoint is saved in checkpoints/clara_epoch_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4945/4945 [1:14:59<00:00,  1.10it/s, L=0.475, A=0.2, B=1]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation for epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6533, Val ABSA: 0.4601\n",
      "Epoch 4: checkpoint is saved in checkpoints/clara_epoch_4\n",
      "Logs are saved locally in: research_logs/Phi3.5-MAMS-Joint-Training_1766876526.csv\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab6d7c-11ca-4123-880a-f138375a8b8d",
   "metadata": {},
   "source": [
    "# Оценка метрик модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ed7141-dcf8-4ae6-9f82-97fc453e26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(clara_model, dataloader, device, name=\"Test\"):\n",
    "    clara_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    mapping = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "    rev_mapping = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n",
    "    \n",
    "    print(f\"--- Evaluating {name} (Manual Calculation) ---\")\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        task_types = batch['task']\n",
    "        input_batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        outputs = clara_model(**input_batch, task=task_types)\n",
    "        logits = outputs[\"logits\"]\n",
    "        labels = input_batch[\"labels\"]\n",
    "        \n",
    "        for i in range(len(task_types)):\n",
    "            if task_types[i] == \"reason\":\n",
    "                target_mask = (labels[i] != -100)\n",
    "                if not target_mask.any(): continue\n",
    "                \n",
    "                target_logits = logits[i][target_mask]\n",
    "                preds = torch.argmax(target_logits, dim=-1)\n",
    "                \n",
    "                pred_text = clara_model.tokenizer.decode(preds, skip_special_tokens=True).strip().lower()\n",
    "                label_text = clara_model.tokenizer.decode(labels[i][target_mask], skip_special_tokens=True).strip().lower()\n",
    "                \n",
    "                p_id = mapping.get(pred_text, -1)\n",
    "                l_id = mapping.get(label_text, -1)\n",
    "                \n",
    "                if l_id != -1: # Считаем только если золотая метка корректна\n",
    "                    all_preds.append(p_id)\n",
    "                    all_labels.append(l_id)\n",
    "\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    \n",
    "    # 1. Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # 2. F1-Score per class\n",
    "    f1_scores = []\n",
    "    class_stats = {}\n",
    "    \n",
    "    for class_id in [0, 1, 2]:\n",
    "        tp = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "        fp = np.sum((y_true != class_id) & (y_pred == class_id))\n",
    "        fn = np.sum((y_true == class_id) & (y_pred != class_id))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        class_stats[rev_mapping[class_id]] = {\"P\": precision, \"R\": recall, \"F1\": f1, \"Support\": np.sum(y_true == class_id)}\n",
    "\n",
    "    # 3. Macro F1\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "\n",
    "    # --- ПЕЧАТЬ ОТЧЕТА ---\n",
    "    print(f\"\\nFinal Results for {name}:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Accuracy: {accuracy:.44f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.44f}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"{'Class':<12} | {'P':<6} | {'R':<6} | {'F1':<6} | {'Support'}\")\n",
    "    print(f\"{'-'*45}\")\n",
    "    for cls, s in class_stats.items():\n",
    "        print(f\"{cls:<12} | {s['P']:<6.3f} | {s['R']:<6.3f} | {s['F1']:<6.3f} | {s['Support']}\")\n",
    "    \n",
    "    clara_model.train()\n",
    "    return {\"acc\": accuracy, \"f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbdae49-3f5f-4b27-adb0-074bb2772c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_metrics_pure_python(clara_model, dataloader, device, name=\"Test\"):\n",
    "    clara_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    mapping = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "    rev_mapping = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n",
    "    \n",
    "    print(f\"--- Evaluating {name} (Generation Mode) ---\")\n",
    "    debug_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        task_types = batch['task']\n",
    "        # Нам нужны только задачи 'reason'\n",
    "        indices_to_process = [i for i, t in enumerate(task_types) if t == \"reason\"]\n",
    "        if not indices_to_process: continue\n",
    "\n",
    "        # Перенос на GPU\n",
    "        enc_ids = batch[\"enc_input_ids\"].to(device)\n",
    "        enc_mask = batch[\"enc_mask\"].to(device)\n",
    "        dec_ids = batch[\"dec_input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # 1. Получаем векторы памяти из энкодера\n",
    "        memory_states, _ = clara_model.get_encoder_memory_states(enc_ids, enc_mask)\n",
    "\n",
    "        # 2. Готовим вход для генерации (только промпт без ответов)\n",
    "        # Нам нужно отрезать от dec_ids ту часть, где начинаются лейблы (ответы)\n",
    "        for i in indices_to_process:\n",
    "            # Находим, где начинается ответ (первый индекс, где label != -100)\n",
    "            label_mask = (labels[i] != -100)\n",
    "            if not label_mask.any(): continue\n",
    "            \n",
    "            # Индекс начала ответа\n",
    "            start_idx = label_mask.nonzero()[0].item()\n",
    "            \n",
    "            # Промпт для генерации: Memory Tokens + Текст промпта\n",
    "            # Берем из dec_ids всё ДО начала ответа\n",
    "            prompt_ids = dec_ids[i:i+1, clara_model.num_mem_tokens : start_idx]\n",
    "            prompt_embeds = clara_model.model.get_input_embeddings()(prompt_ids)\n",
    "            \n",
    "            # Конкатенируем с памятью конкретно этого примера\n",
    "            # memory_states[i:i+1] -> [1, num_mem, D]\n",
    "            current_mem = memory_states[i:i+1]\n",
    "            inputs_embeds = torch.cat([current_mem, prompt_embeds], dim=1)\n",
    "\n",
    "            # 3. ГЕНЕРАЦИЯ (модель сама решит, какой токен выдать)\n",
    "            gen_outputs = clara_model.model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                max_new_tokens=3, # \"positive\" это 1-2 токена\n",
    "                pad_token_id=clara_model.tokenizer.pad_token_id,\n",
    "                eos_token_id=clara_model.tokenizer.eos_token_id,\n",
    "                do_sample=False # Жадный поиск для воспроизводимости\n",
    "            )\n",
    "            \n",
    "            # Декодируем\n",
    "            pred_text = clara_model.tokenizer.decode(gen_outputs[0], skip_special_tokens=True).strip().lower()\n",
    "            \n",
    "            # Берем золотую метку\n",
    "            label_ids = labels[i][label_mask]\n",
    "            label_text = clara_model.tokenizer.decode(label_ids, skip_special_tokens=True).strip().lower()\n",
    "\n",
    "            # Сопоставляем\n",
    "            p_id = -1\n",
    "            for word, idx in mapping.items():\n",
    "                if word in pred_text:\n",
    "                    p_id = idx\n",
    "                    break\n",
    "            \n",
    "            l_id = mapping.get(label_text, -1)\n",
    "\n",
    "            if debug_count < 5:\n",
    "                print(f\"DEBUG | Gold: '{label_text}' | Pred: '{pred_text}' (ID: {p_id})\")\n",
    "                debug_count += 1\n",
    "\n",
    "            if l_id != -1:\n",
    "                all_preds.append(p_id)\n",
    "                all_labels.append(l_id)\n",
    "\n",
    "    # --- МАТЕМАТИКА (Accuracy / F1) ---\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    \n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    f1_scores = []\n",
    "    for cid in [0, 1, 2]:\n",
    "        tp = np.sum((y_true == cid) & (y_pred == cid))\n",
    "        fp = np.sum((y_true != cid) & (y_pred == cid))\n",
    "        fn = np.sum((y_true == cid) & (y_pred != cid))\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_scores.append(2 * (p * r) / (p + r) if (p + r) > 0 else 0)\n",
    "\n",
    "    print(f\"\\nFinal Results: Acc: {accuracy:.4f} | Macro F1: {np.mean(f1_scores):.4f}\")\n",
    "    return {\"acc\": accuracy, \"f1\": np.mean(f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd434b4b-fcdf-40f8-8da1-d720a6afa22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test reasoning examples for ABSA: 1336\n",
      "--- Evaluating MAMS Test Set (Reasoning Only) (Generation Mode) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG | Gold: 'neutral' | Pred: 'positive' (ID: 0)\n",
      "DEBUG | Gold: 'positive' | Pred: 'positive' (ID: 0)\n",
      "DEBUG | Gold: 'positive' | Pred: 'neutral' (ID: 2)\n",
      "DEBUG | Gold: 'neutral' | Pred: 'negative' (ID: 1)\n",
      "DEBUG | Gold: 'negative' | Pred: 'negative' (ID: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [01:51<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results: Acc: 0.6774 | Macro F1: 0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.base_config import ClaraConfig\n",
    "from src.data_utils import MamsClaraDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = ClaraConfig() \n",
    "\n",
    "\n",
    "\n",
    "full_test_ds = MamsClaraDataset(\n",
    "    \"../data/test.xml\", \n",
    "    tokenizer, \n",
    "    num_mem_tokens=config.num_mem_tokens, \n",
    "    max_enc_len=config.max_enc_len, \n",
    "    max_dec_len=config.max_dec_len\n",
    ")\n",
    "\n",
    "reason_only_samples = [s for s in full_test_ds.samples if s['task'] == \"reason\"]\n",
    "full_test_ds.samples = reason_only_samples\n",
    "\n",
    "print(f\"Number of test reasoning examples for ABSA: {len(full_test_ds)}\")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    full_test_ds,  \n",
    "    batch_size=config.val_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_results = evaluate_metrics_pure_python(model, test_loader, device, name=\"MAMS Test Set (Reasoning Only)\")\n",
    "\n",
    "# train_ds = MamsClaraDataset(\"data/train.xml\", tokenizer, ...)\n",
    "# train_loader = DataLoader(train_ds, batch_size=config.val_batch_size, shuffle=False)\n",
    "# train_results = evaluate_metrics(clara_model, train_loader, device, name=\"MAMS Train Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec2f3d8-f669-435a-b2f4-3d392376730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reasoning examples for ABSA: 1336\n",
      "--- Evaluating MAMS Test Set (Reasoning Only) (Generation Mode) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG | Gold: 'negative' | Pred: 'negative' (ID: 1)\n",
      "DEBUG | Gold: 'positive' | Pred: 'positive' (ID: 0)\n",
      "DEBUG | Gold: 'positive' | Pred: 'negative' (ID: 1)\n",
      "DEBUG | Gold: 'neutral' | Pred: 'neutral' (ID: 2)\n",
      "DEBUG | Gold: 'negative' | Pred: 'negative' (ID: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [15:32<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results: Acc: 0.7615 | Macro F1: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_train_ds = MamsClaraDataset(\n",
    "    \"../data/train.xml\", \n",
    "    tokenizer, \n",
    "    num_mem_tokens=config.num_mem_tokens, \n",
    "    max_enc_len=config.max_enc_len, \n",
    "    max_dec_len=config.max_dec_len\n",
    ")\n",
    "\n",
    "train_reason_only_samples = [s for s in full_train_ds.samples if s['task'] == \"reason\"]\n",
    "full_train_ds.samples = train_reason_only_samples\n",
    "\n",
    "print(f\"Number of train reasoning examples for ABSA: {len(full_test_ds)}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    full_train_ds,  \n",
    "    batch_size=config.val_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_results = evaluate_metrics_pure_python(model, train_loader, device, name=\"MAMS Test Set (Reasoning Only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc4ba7-6625-4615-9a44-95b8482b046d",
   "metadata": {},
   "source": [
    "# Тестирование инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ee2c748-8bb6-4cfc-8155-283d6ca77d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_absa_v4(clara_model, text, aspect, device):\n",
    "    clara_model.eval()\n",
    "    tokenizer = clara_model.tokenizer\n",
    "    \n",
    "    # 1. ENCODER: Сжимаем текст\n",
    "    # Важно: используем ту же строку, что в Dataset\n",
    "    mem_tokens_str = \" \".join([f\"[M{i}]\" for i in range(clara_model.num_mem_tokens)])\n",
    "    enc_text = f\"{text} {mem_tokens_str}\"\n",
    "    enc_res = tokenizer(enc_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    memory_states, _ = clara_model.get_encoder_memory_states(enc_res.input_ids, enc_res.attention_mask)\n",
    "    \n",
    "    # 2. DECODER: Повторяем логику подготовки dec_input_ids из Dataset\n",
    "    # Строка: \"[M0] [M1] ... [M7] Sentiment of {aspect}?\"\n",
    "    dec_prompt_text = f\"{mem_tokens_str} Sentiment of {aspect}?\"\n",
    "    \n",
    "    # tokenizer добавит BOS автоматически, как и при обучении\n",
    "    dec_res = tokenizer(dec_prompt_text, return_tensors=\"pt\").to(device)\n",
    "    dec_input_ids = dec_res.input_ids\n",
    "    \n",
    "    # ПОВТОРЯЕМ СРЕЗ ИЗ ТВОЕГО FORWARD:\n",
    "    # Отрезаем первые num_mem_tokens (т.е. BOS и M0-M6)\n",
    "    rest_of_dec_ids = dec_input_ids[:, clara_model.num_mem_tokens:]\n",
    "    rest_of_dec_embeds = clara_model.model.get_input_embeddings()(rest_of_dec_ids)\n",
    "    \n",
    "    # СКЛЕИВАЕМ (как в твоем forward)\n",
    "    # 8 векторов из энкодера + остаток (начиная с M7 и промпта)\n",
    "    inputs_embeds = torch.cat([memory_states, rest_of_dec_embeds], dim=1)\n",
    "    \n",
    "    # Маска внимания\n",
    "    attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=device)\n",
    "    \n",
    "    # 3. GENERATION\n",
    "    outputs = clara_model.model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f3c1608-a174-4d54-bcc1-a4e9f534804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza: positive\n",
      "Manager: positive\n",
      "Waitstaff: negative\n"
     ]
    }
   ],
   "source": [
    "text = \"The pizza was absolutely delicious and hot, but the manager was incredibly rude even though the waitstaff tried their best to be helpful.\"\n",
    "\n",
    "print(f\"Pizza: {predict_absa_v4(model, text, 'pizza', device)}\")\n",
    "print(f\"Manager: {predict_absa_v4(model, text, 'manager', device)}\")\n",
    "print(f\"Waitstaff: {predict_absa_v4(model, text, 'waitstaff', device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc70f10-4177-4888-86f1-f352bf6842e7",
   "metadata": {},
   "source": [
    "# Расчет коэффициента сжатия текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f8a3781-0cd6-48e4-9b5c-4161be3fa9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Статистика сжатия MAMS ---\n",
      "Средняя длина текста: 33.31 токенов\n",
      "Максимальная длина:  114 токенов\n",
      "Минимальная длина:   5 токенов\n",
      "Используется памяти: 8 токенов\n",
      "==============================\n",
      "Коэффициент сжатия (CR): 4.16x\n"
     ]
    }
   ],
   "source": [
    "def calculate_compression_stats(dataset, tokenizer, num_mem_tokens):\n",
    "    token_lengths = []\n",
    "    \n",
    "    for sample in dataset.samples:\n",
    "        # Считаем только уникальные тексты (в MAMS один текст дублируется для каждого аспекта)\n",
    "        # Но для статистики датасета можно считать и все вхождения\n",
    "        tokens = tokenizer.encode(sample['text'], add_special_tokens=False)\n",
    "        token_lengths.append(len(tokens))\n",
    "    \n",
    "    avg_len = np.mean(token_lengths)\n",
    "    max_len = np.max(token_lengths)\n",
    "    min_len = np.min(token_lengths)\n",
    "    \n",
    "    # Коэффициент сжатия: средняя длина / количество векторов памяти\n",
    "    compression_ratio = avg_len / num_mem_tokens\n",
    "    \n",
    "    print(f\"--- Статистика сжатия MAMS ---\")\n",
    "    print(f\"Средняя длина текста: {avg_len:.2f} токенов\")\n",
    "    print(f\"Максимальная длина:  {max_len} токенов\")\n",
    "    print(f\"Минимальная длина:   {min_len} токенов\")\n",
    "    print(f\"Используется памяти: {num_mem_tokens} токенов\")\n",
    "    print(f\"{'='*30}\")\n",
    "    print(f\"Коэффициент сжатия (CR): {compression_ratio:.2f}x\")\n",
    "    \n",
    "    return avg_len, compression_ratio\n",
    "\n",
    "# Запуск\n",
    "new_train_ds = MamsClaraDataset(\n",
    "    \"../data/train.xml\", \n",
    "    tokenizer, \n",
    "    num_mem_tokens=config.num_mem_tokens, \n",
    "    max_enc_len=config.max_enc_len, \n",
    "    max_dec_len=config.max_dec_len\n",
    ")\n",
    "avg_l, cr = calculate_compression_stats(new_train_ds, tokenizer, config.num_mem_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-byakubson-nlp-absa]",
   "language": "python",
   "name": "conda-env-.conda-byakubson-nlp-absa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
